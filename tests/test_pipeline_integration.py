"""
Integration tests for the full pipeline: generation → parquet → dataset → dataloader.

These tests verify that:
1. generate_rollouts.py produces valid parquet files
2. Parquet files have the expected schema for both training modes
3. PreTokenizedSFTDataset correctly loads the data
4. DataLoader batching works correctly and preserves alignment
5. All tensor shapes and data types are correct

Requirements:
- SGLang server must be running (default: localhost:31234)
- Tests are marked as expensive
"""

import json
import os
import shutil
import subprocess
import sys
from pathlib import Path
from typing import Dict, Any

import numpy as np
import pandas as pd
import pytest
import torch
from torch.utils.data import DataLoader

# Add project paths
PROJECT_ROOT = Path(__file__).parent.parent
SCRIPTS_DIR = PROJECT_ROOT / "scripts"
if str(SCRIPTS_DIR) not in sys.path:
    sys.path.insert(0, str(SCRIPTS_DIR))

# Import after path setup
sys.path.insert(0, str(PROJECT_ROOT / "verl"))
from verl.utils.dataset.pretokenized_sft_dataset import PreTokenizedSFTDataset


# Mark all tests in this module as expensive
pytestmark = pytest.mark.expensive


@pytest.fixture(scope="module")
def sglang_server_url():
    """Get SGLang server URL from environment or use default."""
    return os.environ.get("SGLANG_SERVER_URL", "http://localhost:31234")


@pytest.fixture
def temp_output_dir(tmp_path):
    """Create a temporary directory for test outputs."""
    output_dir = tmp_path / "integration_test"
    output_dir.mkdir(exist_ok=True)
    return output_dir


@pytest.fixture
def generated_parquet(temp_output_dir, sglang_server_url):
    """Generate a small set of rollouts and return the parquet file path.

    Runs generate_rollouts.py with minimal settings:
    - 2 unique games
    - group_size=2 (4 total games, 8 sequences)
    - Short conversations to speed up generation
    - Uses fine-tuned checkpoint (faster than base model)
    """
    output_file = temp_output_dir / "test_rollouts.parquet"

    # Use the fine-tuned checkpoint which should generate faster
    checkpoint_path = "/home/nickatomlin/georgiazhou/self_play/checkpoints/sft_qwen3_8b/global_step_3600_merged"

    # Run generate_rollouts.py
    cmd = [
        "python", "scripts/generate_rollouts.py",
        "--num-games", "2",  # Reduced to 2 unique games
        "--group-size", "2",
        "--server-url", sglang_server_url,
        "--model-id", checkpoint_path,  # Use fine-tuned checkpoint
        "--temperature", "0.7",
        "--max-model-len", "4096",
        "--out", str(output_file),
        "--num-procs", "2",  # Reduce parallelism for test
        "--threads-per-proc", "2",
        "--max-turns", "3",  # Very short conversations
        "--seed", "42",
    ]

    print(f"\nRunning generate_rollouts.py: {' '.join(cmd)}")
    print(f"Using checkpoint: {checkpoint_path}")
    print(f"This may take up to 10 minutes...")

    try:
        result = subprocess.run(
            cmd,
            cwd=PROJECT_ROOT,
            capture_output=True,
            text=True,
            timeout=600,  # 10 minute timeout (Qwen3-8B is slow)
            check=True,
        )
        print(f"Generation completed successfully")
        print(f"Output file: {output_file}")

        # Verify file was created
        assert output_file.exists(), f"Output file not created: {output_file}"

        return output_file

    except subprocess.TimeoutExpired:
        pytest.fail("generate_rollouts.py timed out after 3 minutes")
    except subprocess.CalledProcessError as e:
        pytest.fail(f"generate_rollouts.py failed:\nSTDOUT:\n{e.stdout}\n\nSTDERR:\n{e.stderr}")
    except Exception as e:
        pytest.fail(f"Failed to run generate_rollouts.py: {e}")


class TestParquetSchema:
    """Test that generated parquet files have the expected schema."""

    def test_schema_expert_iteration_mode(self, generated_parquet):
        """Test schema for Expert Iteration (filtered BC) mode.

        Expected columns (as generated by generate_rollouts.py):
        - input_ids: list[int] - tokenized input sequence
        - attention_mask: list[int] - attention mask
        - position_ids: list[int] - position IDs
        - loss_mask: list[int] - mask indicating which tokens to train on
        - sample_weight: float - GRPO-normalized reward
        - game_id: int - game identifier for grouping
        - policy_logprobs: str (JSON) or list[float] - logprobs from policy
        - game_normalized_reward: float - normalized game reward
        - game_info: str (JSON) - game metadata
        - full_conversation: str (JSON) - conversation history
        """
        # Load parquet
        df = pd.read_parquet(generated_parquet)

        print(f"\n✓ Loaded parquet with {len(df)} rows")
        print(f"  Columns: {list(df.columns)}")

        # Check required columns exist
        required_columns = [
            "input_ids",
            "attention_mask",
            "position_ids",
            "loss_mask",
            "sample_weight",
            "game_id",
            "policy_logprobs",
            "game_normalized_reward",
            "game_info",
            "full_conversation",
        ]

        missing_columns = [col for col in required_columns if col not in df.columns]
        assert len(missing_columns) == 0, (
            f"Missing required columns: {missing_columns}"
        )

        # Check data types
        sample_row = df.iloc[0]

        # Token arrays should be list-like (can be list or numpy array after parquet round-trip)
        assert hasattr(sample_row["input_ids"], "__len__"), "input_ids should be array-like"
        assert hasattr(sample_row["attention_mask"], "__len__"), "attention_mask should be array-like"
        assert hasattr(sample_row["position_ids"], "__len__"), "position_ids should be array-like"
        assert hasattr(sample_row["loss_mask"], "__len__"), "loss_mask should be array-like"

        # Scalar values
        assert isinstance(sample_row["sample_weight"], (float, int)), "sample_weight should be numeric"
        assert isinstance(sample_row["game_id"], (int, np.integer)), "game_id should be integer"
        assert isinstance(sample_row["game_normalized_reward"], (float, int)), "game_normalized_reward should be numeric"

        # policy_logprobs can be JSON string or list
        policy_logprobs = sample_row["policy_logprobs"]
        if isinstance(policy_logprobs, str):
            # Should be valid JSON
            try:
                parsed = json.loads(policy_logprobs)
                assert isinstance(parsed, list), "policy_logprobs JSON should parse to list"
            except json.JSONDecodeError:
                pytest.fail(f"policy_logprobs is not valid JSON: {policy_logprobs[:100]}")
        else:
            assert hasattr(policy_logprobs, "__len__"), "policy_logprobs should be array-like"

        # JSON strings
        assert isinstance(sample_row["game_info"], str), "game_info should be string"
        assert isinstance(sample_row["full_conversation"], str), "full_conversation should be string"

        # Verify JSON fields are valid
        try:
            json.loads(sample_row["game_info"])
            json.loads(sample_row["full_conversation"])
        except json.JSONDecodeError as e:
            pytest.fail(f"Invalid JSON in game_info or full_conversation: {e}")

        print(f"\n✓ Schema validation passed for Expert Iteration mode:")
        print(f"  - All required columns present")
        print(f"  - Data types correct")
        print(f"  - Sample row input_ids length: {len(sample_row['input_ids'])}")
        print(f"  - Sample row loss_mask sum: {sum(sample_row['loss_mask'])}")

    def test_schema_ppo_grpo_mode(self, generated_parquet):
        """Test that parquet has the fields needed for PPO/GRPO mode.

        PPO mode requires:
        - policy_logprobs: for computing advantages with old policy
        - game_id: for computing group_index (game_id // group_size)

        When loaded by PreTokenizedSFTDataset with use_ppo_loss=True:
        - policy_logprobs becomes old_log_probs tensor
        - game_id is used to compute group_index
        """
        df = pd.read_parquet(generated_parquet)

        # Check PPO-specific columns
        assert "policy_logprobs" in df.columns, "Missing policy_logprobs for PPO mode"
        assert "game_id" in df.columns, "Missing game_id for PPO grouping"

        # Verify policy_logprobs has content
        sample_logprobs = df.iloc[0]["policy_logprobs"]
        if isinstance(sample_logprobs, str):
            parsed_logprobs = json.loads(sample_logprobs)
        else:
            parsed_logprobs = sample_logprobs

        assert len(parsed_logprobs) > 0, "policy_logprobs should not be empty"

        # Verify game_id allows grouping
        game_ids = df["game_id"].values
        assert len(game_ids) > 0, "No game_ids found"
        print(f"\n✓ PPO/GRPO schema validation passed:")
        print(f"  - policy_logprobs present with {len(parsed_logprobs)} values in sample")
        print(f"  - game_id range: {game_ids.min()} to {game_ids.max()}")
        print(f"  - Unique games: {len(set(game_ids))}")


class TestIntegration:
    """Test the full pipeline from generation to DataLoader."""

    def test_dataset_loading_expert_iteration(self, generated_parquet):
        """Test loading parquet with PreTokenizedSFTDataset in Expert Iteration mode."""

        # Create dataset config for Expert Iteration (no PPO)
        config = {
            "max_length": 4096,
            "trainer": {
                "use_ppo_loss": False,
            }
        }

        # Load dataset
        dataset = PreTokenizedSFTDataset(
            parquet_files=[str(generated_parquet)],
            tokenizer=None,  # Not used for pretokenized data
            config=config,
        )

        print(f"\n✓ Loaded dataset with {len(dataset)} samples")

        # Check dataset properties
        assert len(dataset) > 0, "Dataset is empty"
        assert dataset.has_weight, "Dataset should have sample_weight column"
        assert not dataset.use_ppo_loss, "Should be in Expert Iteration mode (not PPO)"

        # Get a sample
        sample = dataset[0]

        # Check sample structure
        required_keys = ["input_ids", "attention_mask", "position_ids", "loss_mask", "sample_weight"]
        missing_keys = [key for key in required_keys if key not in sample]
        assert len(missing_keys) == 0, f"Missing keys in sample: {missing_keys}"

        # Check tensor types and shapes
        assert isinstance(sample["input_ids"], torch.Tensor), "input_ids should be tensor"
        assert isinstance(sample["attention_mask"], torch.Tensor), "attention_mask should be tensor"
        assert isinstance(sample["position_ids"], torch.Tensor), "position_ids should be tensor"
        assert isinstance(sample["loss_mask"], torch.Tensor), "loss_mask should be tensor"
        assert isinstance(sample["sample_weight"], float), "sample_weight should be float scalar"

        # Check shapes match
        seq_len = sample["input_ids"].shape[0]
        assert sample["attention_mask"].shape[0] == seq_len, "attention_mask length mismatch"
        assert sample["loss_mask"].shape[0] == seq_len, "loss_mask length mismatch"

        # Position IDs can be 1D or 2D
        if sample["position_ids"].dim() == 1:
            assert sample["position_ids"].shape[0] == seq_len, "position_ids length mismatch"

        print(f"✓ Sample structure validated:")
        print(f"  - Sequence length: {seq_len}")
        print(f"  - Masked tokens: {sample['loss_mask'].sum().item()}")
        print(f"  - Sample weight: {sample['sample_weight']:.4f}")

    def test_dataset_loading_ppo_grpo(self, generated_parquet):
        """Test loading parquet with PreTokenizedSFTDataset in PPO/GRPO mode."""

        # Create dataset config for PPO
        config = {
            "max_length": 4096,
            "trainer": {
                "use_ppo_loss": True,
                "group_size": 2,  # Must match the group_size used in generation
            }
        }

        # Load dataset
        dataset = PreTokenizedSFTDataset(
            parquet_files=[str(generated_parquet)],
            tokenizer=None,
            config=config,
        )

        print(f"\n✓ Loaded PPO dataset with {len(dataset)} samples")

        # Check dataset properties
        assert dataset.use_ppo_loss, "Should be in PPO mode"
        assert dataset.group_size == 2, "Group size mismatch"

        # Get a sample
        sample = dataset[0]

        # Check PPO-specific fields
        assert "old_log_probs" in sample, "Missing old_log_probs in PPO mode"
        assert "group_index" in sample, "Missing group_index in PPO mode"

        # Check types
        assert isinstance(sample["old_log_probs"], torch.Tensor), "old_log_probs should be tensor"
        assert isinstance(sample["group_index"], (int, torch.Tensor)), "group_index should be int or tensor"

        # Check alignment (lengths should match when no truncation applied)
        seq_len = sample["input_ids"].shape[0]
        assert sample["old_log_probs"].shape[0] == seq_len, (
            f"old_log_probs length {sample['old_log_probs'].shape[0]} != seq_len {seq_len}"
        )

        print(f"✓ PPO sample structure validated:")
        print(f"  - Sequence length: {seq_len}")
        print(f"  - old_log_probs shape: {sample['old_log_probs'].shape}")
        print(f"  - group_index: {sample['group_index']}")

    def test_dataloader_batching_expert_iteration(self, generated_parquet):
        """Test that DataLoader correctly batches data in Expert Iteration mode."""

        config = {
            "max_length": 4096,
            "trainer": {"use_ppo_loss": False}
        }

        dataset = PreTokenizedSFTDataset(
            parquet_files=[str(generated_parquet)],
            tokenizer=None,
            config=config,
        )

        # Create DataLoader with batch_size=4
        batch_size = 4
        dataloader = DataLoader(
            dataset,
            batch_size=min(batch_size, len(dataset)),
            shuffle=False,
            num_workers=0,
        )

        # Get first batch
        batch = next(iter(dataloader))

        actual_batch_size = batch["input_ids"].shape[0]
        seq_len = batch["input_ids"].shape[1]

        print(f"\n✓ DataLoader batching successful:")
        print(f"  - Batch size: {actual_batch_size}")
        print(f"  - Sequence length: {seq_len}")

        # Check batch structure
        assert batch["input_ids"].shape == (actual_batch_size, seq_len), "input_ids shape mismatch"
        assert batch["attention_mask"].shape == (actual_batch_size, seq_len), "attention_mask shape mismatch"
        assert batch["loss_mask"].shape == (actual_batch_size, seq_len), "loss_mask shape mismatch"
        assert batch["sample_weight"].shape == (actual_batch_size,), "sample_weight should be 1D"

        # Verify alignment is preserved across batch
        for i in range(actual_batch_size):
            masked_positions = batch["loss_mask"][i].sum().item()
            assert masked_positions > 0, f"Sample {i} has no masked positions"

        print(f"  - All samples have masked positions")
        print(f"  - Masked tokens per sample: {[batch['loss_mask'][i].sum().item() for i in range(actual_batch_size)]}")

    def test_dataloader_batching_ppo_grpo(self, generated_parquet):
        """Test that DataLoader correctly batches data in PPO/GRPO mode."""

        config = {
            "max_length": 4096,
            "trainer": {
                "use_ppo_loss": True,
                "group_size": 2,
            }
        }

        dataset = PreTokenizedSFTDataset(
            parquet_files=[str(generated_parquet)],
            tokenizer=None,
            config=config,
        )

        batch_size = 4
        dataloader = DataLoader(
            dataset,
            batch_size=min(batch_size, len(dataset)),
            shuffle=False,
            num_workers=0,
        )

        batch = next(iter(dataloader))

        actual_batch_size = batch["input_ids"].shape[0]
        seq_len = batch["input_ids"].shape[1]

        print(f"\n✓ PPO DataLoader batching successful:")
        print(f"  - Batch size: {actual_batch_size}")
        print(f"  - Sequence length: {seq_len}")

        # Check PPO-specific batch structure
        assert "old_log_probs" in batch, "Missing old_log_probs in batch"
        assert "group_index" in batch, "Missing group_index in batch"

        assert batch["old_log_probs"].shape == (actual_batch_size, seq_len), (
            f"old_log_probs shape {batch['old_log_probs'].shape} != expected ({actual_batch_size}, {seq_len})"
        )
        assert batch["group_index"].shape == (actual_batch_size,), (
            f"group_index should be 1D with shape ({actual_batch_size},), got {batch['group_index'].shape}"
        )

        # Verify group indices are valid
        group_indices = batch["group_index"].cpu().numpy()
        print(f"  - Group indices in batch: {group_indices}")

        # Group indices should be non-negative integers
        assert all(g >= 0 for g in group_indices), "Group indices should be non-negative"

        print(f"  - All group indices valid")
