hydra:
  searchpath:
    - file://verl/trainer/config

defaults:
  - ppo_trainer
  - _self_

data:
  max_prompt_length: 512
  max_response_length: 256
  train_batch_size: 4
  val_batch_size: 4
  return_raw_chat: True
  train_files: ["/home/nickatomlin/georgiazhou/self_play/data/train.parquet"]
  val_files: ["/home/nickatomlin/georgiazhou/self_play/data/train.parquet"]

algorithm:
  adv_estimator: grpo
  norm_adv_by_std_in_grpo: True
  use_kl_in_reward: False

actor_rollout_ref:
  hybrid_engine: True
  
  model:
    path: /home/nickatomlin/georgiazhou/self_play/old/save_points/global_step_1000_merged
    trust_remote_code: True
    enable_gradient_checkpointing: True
    
  actor:
    optim:
      lr: 5e-7
    ppo_mini_batch_size: 4
    ppo_micro_batch_size_per_gpu: 1
    use_kl_loss: True
    fsdp_config:
      model_dtype: float16
  
  # Use standard SGLang rollout instead of custom
  rollout:
    name: sglang
    gpu_memory_utilization: 0.7
    tensor_model_parallel_size: 1
    log_prob_micro_batch_size_per_gpu: 1
    trust_remote_code: True
    
  ref:
    log_prob_micro_batch_size_per_gpu: 1

reward_model:
  enable: False

trainer:
  logger: ["console"]
  project_name: 'test'
  experiment_name: 'no_custom_rollout'
  n_gpus_per_node: 4
  nnodes: 1
  save_freq: -1
  test_freq: -1
  val_before_train: False
  total_epochs: 1
